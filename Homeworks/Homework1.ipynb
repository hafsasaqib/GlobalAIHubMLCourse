{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question1:\n",
    "    How would you define Machine Learning?\n",
    "\n",
    "Answer:\n",
    "    Machine learning is a subset of artificial intelligence (AI) that seeks to analyze patterns in data to advance \n",
    "    decision-making and learning. Though we might not notice machine learning, it’s everywhere in our lives. When we \n",
    "    browse sites like Netflix or YouTube, we are recommended certain videos and shows to watch. That’s machine learning.\n",
    "    Machine learning focuses on the development of computer programs that can access data and use it to learn for \n",
    "    themselves. The process of learning begins with observations or data, such as examples, direct experience, or \n",
    "    instruction, in order to look for patterns in data and make better decisions in the future based on the examples that\n",
    "    we provide. The primary aim is to allow the computers learn automatically without human intervention or assistance and\n",
    "    adjust actions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question2:\n",
    "    What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these.\n",
    "\n",
    "Answer:\n",
    "    \n",
    "    * Supervised Learning:\n",
    "        Supervised machine learning algorithms can apply what has been learned in the past to new data using labeled \n",
    "        examples to predict future events. Starting from the analysis of a known training dataset, the learning algorithm \n",
    "        produces an inferred function to make predictions about the output values. The system is able to provide targets \n",
    "        for any new input after sufficient training. The learning algorithm can also compare its output with the correct, \n",
    "        intended output and find errors in order to modify the model accordingly.\n",
    "        * Example:\n",
    "            1. In supervised learning is text classification problems. In this set of problems, the goal is to predict the \n",
    "            class label of a given piece of text. One particularly popular topic in text classification is to predict the\n",
    "            sentiment of a piece of text, like a tweet or a product review. This is widely used in the e-commerce industry\n",
    "            to help companies to determine negative comments made by customers.    \n",
    "            2. One practical example of supervised learning problems is predicting house prices. How is this achieved?\n",
    "            First, we need data about the houses: square footage, number of rooms, features, whether a house has a garden \n",
    "            or not, and so on. We then need to know the prices of these houses, i.e. the corresponding labels. By \n",
    "            leveraging data coming from thousands of houses, their features and prices, we can now train a supervised \n",
    "            machine learning model to predict a new house’s price based on the examples observed by the model. \n",
    "        \n",
    "    * UnsuperVised Learning:\n",
    "        unsupervised machine learning algorithms are used when the information used to train is neither classified nor \n",
    "        labeled. Unsupervised learning studies how systems can infer a function to describe a hidden structure from \n",
    "        unlabeled data. The system doesn’t figure out the right output, but it explores the data and can draw inferences \n",
    "        from datasets to describe hidden structures from unlabeled data.\n",
    "        * Example:\n",
    "            1. Clustering is an unsupervised technique where the goal is to find natural groups or clusters in a feature \n",
    "            spaceand interpret the input data. There are many different clustering algorithms. One common approach is to \n",
    "            dividethe data points in a way that each data point falls into a group that is similar to other data points in \n",
    "            the same group based on a predefined similarity or distance metric in the feature space.Clustering is commonly\n",
    "            used for determining customer segments in marketing data. Being able to determine different segments of \n",
    "            customers helps marketing teams approach these customer segments in unique ways. (Think of features like \n",
    "            gender, location, age, education, income bracket, and so on.)\n",
    "            2. Even though feature selection and dimensionality reduction aim towards reducing the number of features in \n",
    "            the original set of features, understanding how feature selection works helps us get a better understanding of \n",
    "            dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question3: \n",
    "    What are the test and validation set, and why would you want to use them?\n",
    "    \n",
    "Answer:\n",
    "    \n",
    "    * Validation Dataset:\n",
    "        The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model\n",
    "        hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the \n",
    "        model configuration. The validation phase is often split into two parts:\n",
    "        1. In the first part, you just look at your models and select the best performing approach using the validation \n",
    "        data (=validation).\n",
    "        2. Then you estimate the accuracy of the selected approach (=test).\n",
    "        \n",
    "    * Test Dataset:\n",
    "        The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. Ideally,\n",
    "        the test set should be kept in a “vault,” and be brought out only at the end of the data analysis.\n",
    "    \n",
    "    *Reason of using:\n",
    "        Given a choice of hyperparameter values, you use the training set to train the model. But, how do you set the \n",
    "        values for the hyperparameters? Thats what the validation set is for. You can use it to evaluate the performance \n",
    "        of your model for different combinations of hyperparameter values (e.g. by means of a grid search process) and \n",
    "        keep the best trained model.\n",
    "        But, how does your selected model compares to other different models? Is your neural network performing better \n",
    "        than, lets say, a random forest trained with the same combination of training/test data? You cannot compare based\n",
    "        on the validation set, because that validation set was part of the fitting of your model. You used it to select the\n",
    "        hyperparameter values!\n",
    "        The test set allows you to compare different models in an unbiased way, by basing your comparisons in data that \n",
    "        were not use in any part of your training/hyperparameter selection process.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question4:\n",
    "    What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n",
    "\n",
    "Answer:\n",
    "    \n",
    "    * Main Pre-processing Steps:\n",
    "        1. Acquire the dataset  \n",
    "            To build and develop Machine Learning models, you must first acquire the relevant dataset. This dataset will \n",
    "            be comprised of data gathered from multiple and disparate sources which are then combined in a proper format\n",
    "            to form a dataset. Dataset formats differ according to use cases.\n",
    "        2. Import all the crucial libraries\n",
    "            Since Python is the most extensively used and also the most preferred library by Data Scientists around the \n",
    "            world. The predefined Python libraries can perform specific data preprocessing jobs. The three core Python \n",
    "            libraries used for this data preprocessing in Machine Learning are: Numpy, Pandas, and Matplotlib.\n",
    "        3. Import the dataset\n",
    "            In this step, you need to import the dataset/s that you have gathered for the ML project at hand. However, \n",
    "            before you can import the dataset/s, you must set the current directory as the working directory.\n",
    "        4. Identifying and handling the missing values\n",
    "            In data preprocessing, it is pivotal to identify and correctly handle the missing values, failing to do this,\n",
    "            you might draw inaccurate and faulty conclusions and inferences from the data. Needless to say, this will \n",
    "            hamper your ML project. Basically, there are two ways to handle missing data: Deleting a prticular row and \n",
    "            caulculating the mean.\n",
    "        5. Encoding the categorical data\n",
    "            Categorical data refers to the information that has specific categories within the dataset. In the dataset \n",
    "            cited above, there are two categorical variables – country and purchased. Machine Learning models are primarily\n",
    "            based on mathematical equations. Thus, you can intuitively understand that keeping the categorical data in the\n",
    "            equation will cause certain issues since you would only need numbers in the equations.\n",
    "        6. Splitting the dataset\n",
    "            Every dataset for Machine Learning model must be split into two separate sets – training set and test set. \n",
    "            Training set denotes the subset of a dataset that is used for training the machine learning model. Here, you\n",
    "            are already aware of the output. A test set, on the other hand, is the subset of the dataset that is used for\n",
    "            testing the machine learning model. The ML model uses the test set to predict outcomes. \n",
    "            Usually, the dataset is split into 70:30 ratio or 80:20 ratio. This means that you either take 70% or 80% of \n",
    "            the data for training the model while leaving out the rest 30% or 20%. The splitting process varies according \n",
    "            to the shape and size of the dataset in question. \n",
    "        7. Feature scaling\n",
    "            Feature scaling marks the end of the data preprocessing in Machine Learning. It is a method to standardize the\n",
    "            independent variables of a dataset within a specific range. In other words, feature scaling limits the range of\n",
    "            variables so that you can compare them on common grounds.\n",
    "            \n",
    "    * Why data Preparation is needed?\n",
    "        On a predictive modeling project, machine learning algorithms learn a mapping from input variables to a target \n",
    "        variable.\n",
    "        The most common form of predictive modeling project involves so-called structured data or tabular data. This is\n",
    "        data as it looks in a spreadsheet or a matrix, with rows of examples and columns of features for each example.\n",
    "        We cannot fit and evaluate machine learning algorithms on raw data; instead, we must transform the data to meet\n",
    "        the requirements of individual machine learning algorithms. More than that, we must choose a representation for \n",
    "        the data that best exposes the unknown underlying structure of the prediction problem to the learning algorithms \n",
    "        in order to get the best performance given our available resources on a predictive modeling project.\n",
    "        Given that we have standard implementations of highly parameterized machine learning algorithms in open source \n",
    "        libraries, fitting models has become routine. As such, the most challenging part of each predictive modeling \n",
    "        project is how to prepare the one thing that is unique to the project: the data used for modeling.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question5: \n",
    "    How you can explore countionus and discrete variables?\n",
    "    \n",
    "Answer:\n",
    "    * Discrete Variables\n",
    "        Discrete variables are countable in a finite amount of time. For example, you can count the change in your pocket.\n",
    "        You can count the money in your bank account. You could also count the amount of money in everyone’s bank accounts.\n",
    "        It might take you a long time to count that last item, but the point is—it’s still countable.\n",
    "        \n",
    "    * Continuous Variables\n",
    "        Continuous Variables would (literally) take forever to count. In fact, you would get to “forever” and never finish\n",
    "        counting them. For example, take age. You can’t count “age”. Why not? Because it would literally take forever. \n",
    "        For example, you could be: 25 years, 10 months, 2 days, 5 hours, 4 seconds, 4 milliseconds, 8 nanoseconds, \n",
    "        99 picosends…and so on.\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
